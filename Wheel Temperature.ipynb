{"cells": [{"metadata": {"id": "1283f7279fd34958b343ca189e4bc0ac"}, "cell_type": "markdown", "source": "<a id='business_understanding'></a>\n# 1. Business Understanding\n- The purpose of this machine learning project is to predict machine wheel bearing temperatures in order to prevent machine breakdown\n- There are a number of sensors which can be used as input features\n- This project loosely follows the CRISP-DM methodology and is divided into the following sections: <br>\n[1. Business Understanding](#business_understanding)<br>\n[2. Data Understanding](#data_understanding)<br>\n[3. Data Preparation](#data_preparation)<br>\n[4. Modelling](#modelling)<br>\n[5. Evaluation](#evaluation)<br>\n[6. Deployment](#deployment)<br>"}, {"metadata": {"id": "dbbc205f237648deb8701e3ea907ae67"}, "cell_type": "markdown", "source": "<a id='data_understanding'></a>\n# 2. Data Understanding"}, {"metadata": {"id": "fb7bcc33-6616-4b7d-b2a8-a56686230411"}, "cell_type": "markdown", "source": "## Steps\nThe notebook passes through the following steps in reaching the ultimate end-goal: building an Python model that can be used to detect anomalies in one of the sensors: `Wheel Front Temp Celsius`\n\n[Stage 1 - Read the input files and review data](#read_input)<br>\n[Stage 2 - Transform and collect statistics](#transform_collect_statistics)<br>\n[Stage 3 - Visualize the data](#visualize)<br>\n[Stage 4 - Find correlations in the data](#find_correlations)<br>\n[Stage 5 - Train model](#train_model)<br>"}, {"metadata": {"id": "dc79ce4c-562b-4d0f-b1c6-d4e9a7a724c0"}, "cell_type": "markdown", "source": "## Import required packages\nEven though you can import new packages anywhwere in the notebook, it is common practice move the imports to the beginning of the notebook. This to immediately show the dependencies when someone else opens it.\n\nAdditionally, when you finished working on the notebook:\n* Restart the kernel (Kernel --> Restart)\n* Re-run the notebook (Run --> Run All Cells)\n\nThis is to ensure the notebook will also run if cells are not executed in the exact same sequence when you developed it. Very often, the exploration and analysis of data is an iterative process and you may end up renaming variables, updating dataframe columns. By running the full notebook from start to end and validating the end result, you will avoid errors when you need to use it later."}, {"metadata": {"id": "efb2ca61-97d2-4527-8094-c42fcadf247c"}, "cell_type": "code", "source": "import pandas as pd\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt", "execution_count": null, "outputs": []}, {"metadata": {"id": "4bb34276-80da-4f05-9b24-aaebfe1597d4"}, "cell_type": "markdown", "source": "<a id='read_input'></a>\n## Read input files\nIn this example, the files are directly read from the GitHub repository into a Pandas dataframe. Even though you could upload the file to the notebook server (Watson Studio or other), you will find it as easy to load the data directory from the internet.\n\nThe following 3 files have been made available:\n* readings\n* sensors\n* devices\n\nOnce these files have been loaded into the catalog, each individual file can be imported using the Data tab to the right. "}, {"metadata": {"id": "25e94086e72a421185671d559394628f"}, "cell_type": "code", "source": "readings_df = pd.read_csv('/project_data/data_asset/readings.csv')\nreadings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "eaece9930b52486dad874f933b671c8d"}, "cell_type": "code", "source": "sensors_df = pd.read_csv('/project_data/data_asset/sensor.csv')\nsensors_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "ce6f8614ebda4d39ab26d63e74e05600"}, "cell_type": "code", "source": "devices_df = pd.read_csv('/project_data/data_asset/device.csv')\ndevices_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "748ab13f-75bc-4db8-8e3d-b39af4c8e8f5"}, "cell_type": "markdown", "source": "Now that the data has been loaded into the 3 Pandas dataframes, check that the number of records match the expectations.\n* readings: ~326k entries\n* sensors: 27 entries\n* devices: 4 entries"}, {"metadata": {"id": "4a00a5e5-b628-4159-9b81-1359c4412644"}, "cell_type": "code", "source": "print('Number of readings: {}'.format(len(readings_df)))\nprint('Number of sensors: {}'.format(len(sensors_df)))\nprint('Number of devices: {}'.format(len(devices_df)))", "execution_count": null, "outputs": []}, {"metadata": {"id": "d53ef2b7-0eb5-42ea-bc92-d5e1e4a82003"}, "cell_type": "markdown", "source": "## View the first few rows of every data set\nShow the top 5 rows of every dataframe"}, {"metadata": {"id": "00599caf-ad4f-40e0-8d6e-087bc9255460"}, "cell_type": "code", "source": "readings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "91efc64c-84db-4947-9687-fba6987e7d1d"}, "cell_type": "code", "source": "sensors_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false, "id": "8bda4e77-921e-4153-9b13-1a471a96be88"}, "cell_type": "code", "source": "devices_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "f40106c154bb42c589f6153ea96d1a94"}, "cell_type": "markdown", "source": "<a id='data_preparation'></a>\n# 3. Data Preparation"}, {"metadata": {"id": "088f88ca-745f-4f72-85ee-02399190351d"}, "cell_type": "markdown", "source": "<a id='transform_collect_statistics'></a>\n## Convert epoch timestamp to datetime\nExtend the \"readings\" dataframe with a timestamp column which represents the human readable date and time for the `tsepoch` column. If you haven't noticed yet, the `tsepoch` timestamp is expressed in milliseconds."}, {"metadata": {"id": "1dde664c-0483-41f3-967d-2500fb23673f"}, "cell_type": "code", "source": "readings_df['ts']=pd.to_datetime(readings_df['tsepoch'],unit='ms')\nreadings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "23908a54-2bd0-42fa-836a-cf4d8e13e31b"}, "cell_type": "code", "source": "readings_df['ts'].apply(lambda x: x.day)", "execution_count": null, "outputs": []}, {"metadata": {"id": "17c98d3b-62e1-4d47-aaed-69176af74919"}, "cell_type": "markdown", "source": "### Determine time span of the readings\nNow that you've loaded the data, find the following properties for the readings:\n* The number of seconds that the readings span\n* The lowest timestamp in human readable format\n* The highest timestamp in human readable format"}, {"metadata": {"id": "f40af4e0-b72a-4977-8f94-66d762ffe8d7"}, "cell_type": "code", "source": "start_tsepoch=readings_df['tsepoch'].min()\nend_tsepoch=readings_df['tsepoch'].max()\nstart_time=readings_df['ts'].min()\nend_time=readings_df['ts'].max()\nprint('Sample data set spans readings from {} to {}'.format(start_time,end_time))\nprint('Number of seconds spanned by date set: {}'.format((end_tsepoch-start_tsepoch)/1000))", "execution_count": null, "outputs": []}, {"metadata": {"id": "52d32062-49ae-4381-bdd5-dee982834a2c"}, "cell_type": "markdown", "source": "### Determine number of readings for each sensor\nNot every sensor has the same sample rate. Later you will have to resample the readings to ensure you have a value for every time interval. Find the number of readings for every sensor and display this in a chart.\n\n**Tip**: When using `matplotlib`, don't forget to add an instruction to tell the notebook that charts should be shown in-line."}, {"metadata": {"id": "5774bd87-56c2-462e-9c42-0bbc2d126fcd"}, "cell_type": "code", "source": "sensor_counts=readings_df.groupby('sensor_id').size()", "execution_count": null, "outputs": []}, {"metadata": {"id": "e4b6db6d-8a0c-46e5-b398-9b99aa0da477"}, "cell_type": "code", "source": "%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {"id": "5a0c8172-2c17-4089-ab15-c67a5c730a89"}, "cell_type": "code", "source": "r=sensor_counts.plot(kind='bar')", "execution_count": null, "outputs": []}, {"metadata": {"id": "a86f443f-925a-4378-821d-bd93ba8c3758"}, "cell_type": "markdown", "source": "### Determine statistics for each sensor and write to new \"sensors\" CSV file\nThe `sensor.csv` file contained placeholders for the count, minimum, maximum and mean of every sensor. Not all of these values were populated. You will need the sensor statistics later but you do not necessarily need to store them. If you run the notebook in the cloud, you can consider to try and persist the data in the object store, but retrieving the values in a dataframe is sufficient."}, {"metadata": {"id": "a7450505-021e-49a6-a942-9148d24e1488"}, "cell_type": "code", "source": "sensor_statsdf=readings_df.groupby('sensor_id', as_index=False, group_keys=False)['value'].agg(['count','min','max','mean']).reset_index()\nsensor_statsdf", "execution_count": null, "outputs": []}, {"metadata": {"id": "9786a029-5b79-46ff-912b-cbf484489aaa"}, "cell_type": "markdown", "source": "Once you have retrieved the count, minimum, maximum and mean, join this data with the original sensors dataframe so that you can persist it as a csv file.\n\nThe resulting dataframe should have the following columns:\n* sensor_id\n* description\n* low_value\n* high_value\n* mean_value\n\n**Tip**: To be able to join, you may have to reset the index of the dataframe holding the aggregated values, or you may have to use the index of that dataframe to join with the sensors dataframe."}, {"metadata": {"id": "d84d28ba-3890-4d9c-abb0-efd760401c26"}, "cell_type": "code", "source": "sensors_with_stats_df=sensors_df.merge(sensor_statsdf, left_on='sensor_id', right_on='sensor_id').drop(['low_value','high_value','count'], axis=1)\nsensors_with_stats_df.columns=['sensor_id','description','low_value','high_value','mean_value']\nsensors_with_stats_df.head()\n# We cannot write back to the file system in Watson Studio\n# sensors_with_stats_df.to_csv('sensors_with_stats.csv',index=False)", "execution_count": null, "outputs": []}, {"metadata": {"id": "6800595b-0f2e-42bc-8b90-0da53788426a"}, "cell_type": "markdown", "source": "<a id='visualize'></a>\n## Visualization"}, {"metadata": {"id": "6e2eca64-ab12-4ea1-a02e-1419bf2211e8"}, "cell_type": "markdown", "source": "### Determine which sensors make sense to visualize\nSensors with a constant value can be ignored and should be dropped before visualizing. Use the sensor statistics you retrieved above to determine if sensors have a constant value. Drop the readings of those sensors.\n\n**Tip**: When dropping rows or columns, you may get a warning that the original dataframe could be affected. Use the `copy()` function to make a copy of the original dataframe before deleting rows or columns."}, {"metadata": {"id": "a7a321cb-d58f-4908-812a-9f34f60a53a4"}, "cell_type": "code", "source": "non_zero_sensors_df=sensors_with_stats_df.loc[(sensors_with_stats_df['low_value']!=0) | \\\n                                           (sensors_with_stats_df['high_value']!=0)]\nvar_readings_df=readings_df.copy()[readings_df.sensor_id.isin(non_zero_sensors_df.sensor_id)]\nvar_readings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "f3f69e38-356e-4316-bf4a-ff5bc1480e1a"}, "cell_type": "markdown", "source": "If your code is correct, approximately 321900 readings should remain in the new dataframe."}, {"metadata": {"id": "1ea30650-a036-4086-b9b1-7440e495c23b"}, "cell_type": "code", "source": "len(var_readings_df)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d6da4053-0bc3-4b7e-b6d6-bc242cc41b2a"}, "cell_type": "markdown", "source": "## Plot some of the sensors\nPick a couple of sensor IDs (for example: 14, 27 and 68) and plot the values. Use the human readable timestamp for the x-axis."}, {"metadata": {"id": "0ec4c9f1-7f9e-4bac-83b8-3b30d4b7c9ab"}, "cell_type": "code", "source": "plot_readings_df=var_readings_df.copy()[var_readings_df.sensor_id.isin([14,27,68])]\nplot_readings_df=plot_readings_df.pivot_table(index='ts',columns='sensor_id',values='value').reset_index()\nplot_readings_df.plot(x='ts')", "execution_count": null, "outputs": []}, {"metadata": {"id": "8b0fed99-4779-4db1-ae16-e3f5b66f808d"}, "cell_type": "markdown", "source": "## Down-sample the different readings\nBefore we can start looking at correlations between different sensors, we need to match the timestamps of the readings from the different sensos. Let's try and down-sample the variable readings to create a pivoted dataframe with a value for every sensor and every timestamp.\n\nTo lose as little detail as possible, we will down-sample to 0.2 seconds. This means there will be 864 * 5 readings for every sensor.\n\n**Tip**: Use the pandas `resample` function and `groupby` to down-sample the readings to 200 milliseconds."}, {"metadata": {"id": "04b24352-4e9d-44d4-aa0a-e7edf902c38e"}, "cell_type": "code", "source": "readings_resample_df=var_readings_df.copy().set_index('ts').groupby('sensor_id')['value'].resample('200 ms').min().reset_index()\nreadings_resample_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "7581d68c-f8e7-4534-a026-b38c4d8ef302"}, "cell_type": "code", "source": "readings_resample_df.dropna(inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {"id": "6176e6e7-67d2-41c8-a2ab-0bd651d284a1"}, "cell_type": "markdown", "source": "Check that you more or less have the same number of samples for each sensor now."}, {"metadata": {"id": "cd9c0105-2880-413e-946c-d29829e3b693"}, "cell_type": "code", "source": "readings_resample_df.groupby('sensor_id').size()", "execution_count": null, "outputs": []}, {"metadata": {"id": "789d053f-88d8-421b-91b5-7545017248f8"}, "cell_type": "markdown", "source": "This is what we expected. We can now pivot the table to find correlations."}, {"metadata": {"id": "c36ae780-4a24-44e0-8a58-0f3618e10b5c"}, "cell_type": "markdown", "source": "<a id='find_correlations'></a>\n## Find correlations between sensors\nNow that we have the equivalent number of readings for every sensor, you can find correlcations between the different sensor IDs. You will need to match up the readings for different sensors with each other.\n\n**Tip**: Use the pandas `pivot_table` function to get 1 column for every sensor. Every row will have a timestamp and value for each of the sensors."}, {"metadata": {"id": "a2d26288-64c2-4413-8a06-256d20c6e42d"}, "cell_type": "code", "source": "pivot_readings_df=readings_resample_df.pivot_table(index='ts',columns='sensor_id',values='value').reset_index()\npivot_readings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "9c200f37-7665-4004-9932-1fdb5c04cb7b"}, "cell_type": "markdown", "source": "The column names now reference the `sensor_id` which really is not that meaningful. Before correlating, let's give the columns some meaningful names.\n\n**Tip**: Join the numeric column names with the `sensors` data to retrieve the names. Once you have changed the column names, show the first 5 rows of the resulting dataframe."}, {"metadata": {"id": "6d1fa120-b142-4340-8e53-35b4ba34e055"}, "cell_type": "code", "source": "sensor_cols=pd.DataFrame(pivot_readings_df.columns[1:])\nsensor_cols=sensor_cols.astype('int64')", "execution_count": null, "outputs": []}, {"metadata": {"id": "7fcac0a3-ee56-4d8d-9418-9f28b778e9d9"}, "cell_type": "code", "source": "pivot_readings_df.columns=['timestamp'] + sensor_cols.merge(sensors_with_stats_df)['description'].tolist()\npivot_readings_df.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "be283821-cbf3-45a4-af6b-b1bb50e7009a"}, "cell_type": "markdown", "source": "### Build the correlations table"}, {"metadata": {"id": "c0a50a95-b7f1-4267-86e7-f90f4fe33078"}, "cell_type": "code", "source": "readings_cov=pivot_readings_df.corr()\nreadings_cov", "execution_count": null, "outputs": []}, {"metadata": {"id": "cf0dbd84-166e-4d7a-ad65-f1c664cf90c8"}, "cell_type": "markdown", "source": "### Show correlations in heatmap\nThe above correlation diagram is a little difficult to read. It is better to convert this into a heatmap.\n\n**Tip**: The Seaborn package has some nice heatmaps that are easy to use. https://seaborn.pydata.org/"}, {"metadata": {"id": "647e0fac-705f-4c7d-8ae8-1d8eaf98373f"}, "cell_type": "code", "source": "plt.figure(figsize=(15,8))\nsns.heatmap(readings_cov, annot=True, fmt='.2f',cmap='Reds')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "75de7538-b387-451b-973f-376386529826"}, "cell_type": "markdown", "source": "From the above it is obvious that `Wheel Front Speed RPM` and `Wheel Front Temp Celsius` are strongly correlated, let's plot this in a regression plot."}, {"metadata": {"scrolled": false, "id": "bdb0f3ea-b9b3-46a6-8da0-8a5872585144"}, "cell_type": "code", "source": "plt.figure(figsize=(15,10))\nsns.regplot(pivot_readings_df['Wheel Front Speed RPM'], pivot_readings_df['Wheel Front Temp Celsius'],marker='+')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "4bd532e6-73d6-4db3-913a-c8f91a00bdf0"}, "cell_type": "markdown", "source": "The covariance between the speed and temperature is clearly visible in the chart above."}, {"metadata": {"id": "3b7236829a7f4a9c8617512869b717f6"}, "cell_type": "markdown", "source": "<a id='modelling'></a>\n# 4. Modelling"}, {"metadata": {"id": "4028bc1b-67f6-4c56-bb49-01a8066ad6fe"}, "cell_type": "markdown", "source": "<a id='train_model'></a>\n## Train model on the data\n\nNow that we have found a correlation, let's try to build a model we can use for predictions."}, {"metadata": {"id": "f9aadac5-72cb-4bff-843d-c73146e21f6e"}, "cell_type": "markdown", "source": "### Import additional packages\nYou can choose to import additional packages here, but in the end it is recommended to move all imports to the top of the notebook."}, {"metadata": {"id": "bd9fabec-6af7-4008-a518-37a423b6f62a"}, "cell_type": "code", "source": "# Import pipeline libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline", "execution_count": null, "outputs": []}, {"metadata": {"id": "60a27fbb-cdf1-47a1-9506-26bef693a9fb"}, "cell_type": "code", "source": "# Import algorithms\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet", "execution_count": null, "outputs": []}, {"metadata": {"id": "ec3b4ee8-47f2-4597-a91e-58fdd710e65d"}, "cell_type": "markdown", "source": "### Split the data in a training and test set\nIt is best to start from the pivoted dataframe. If you didn't do so before, you first have remove any NaN values from the overall pivoted dataframe, otherwise the training or testing of the model will fail."}, {"metadata": {"id": "ab59a6aa-87a9-42f8-96d0-0448d605df4a"}, "cell_type": "code", "source": "X= pivot_readings_df.dropna().drop(['timestamp','Wheel Front Temp Celsius'], axis=1)\ny = pivot_readings_df.dropna()['Wheel Front Temp Celsius']", "execution_count": null, "outputs": []}, {"metadata": {"id": "dffbc9c3-5577-4c71-a630-798b4e7bdfd2"}, "cell_type": "code", "source": "print(len(X), len(y))", "execution_count": null, "outputs": []}, {"metadata": {"id": "ece5c7ec-57b0-4e76-a62b-205ab4acbc87"}, "cell_type": "markdown", "source": "Now split the dataframe into a training and test set. Please note that the independent variables (features) and dependent variables (labels) must end up in different dataframes/series.\n\n**Tip**: SciKit Learn has a nice function that will split up a dataframe in training and testing data, and also separate features from labels."}, {"metadata": {"id": "46bbc5f2-b1b6-456d-86bb-489912964e1f"}, "cell_type": "code", "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)", "execution_count": null, "outputs": []}, {"metadata": {"id": "394f731e-0a40-4880-b82c-1f8f158d7acb"}, "cell_type": "code", "source": "print(len(X_train), len(X_test), len(y_train), len(y_test))", "execution_count": null, "outputs": []}, {"metadata": {"id": "6f961ee2-1876-4968-9a0d-a41dc27080e5"}, "cell_type": "markdown", "source": "### Setup Model Pipelines\nUse model pipelines to help select the best performing model. The pipeline dictionary below contains five different algorithms with a range of linear and ensemble methods. "}, {"metadata": {"id": "16c3d0a6-576d-40b3-b74c-93f477961927"}, "cell_type": "code", "source": "pipelines = {\n    'rf':make_pipeline(StandardScaler(), RandomForestRegressor()),\n    'gb':make_pipeline(StandardScaler(), GradientBoostingRegressor()),\n    'ridge':make_pipeline(StandardScaler(), Ridge()),\n    'lasso':make_pipeline(StandardScaler(), Lasso()),\n    'enet':make_pipeline(StandardScaler(), ElasticNet())\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "89600e60-4a92-4001-94b7-9a1550739f84"}, "cell_type": "markdown", "source": "### Specify Model Hyperparameter Grid"}, {"metadata": {"id": "2dea380f-ed9f-4d67-8cca-a05c9e9482bf"}, "cell_type": "code", "source": "mod = RandomForestRegressor()", "execution_count": null, "outputs": []}, {"metadata": {"id": "5df9b17e-319d-45a3-8ef1-21a0d1e50f6d"}, "cell_type": "code", "source": "mod.get_params()", "execution_count": null, "outputs": []}, {"metadata": {"id": "d9c75861-ace3-4ccd-86fc-b648c284402a"}, "cell_type": "code", "source": "hyper = {\n    'rf':{'randomforestregressor__min_samples_split':[2,4,6]},\n    'gb':{'gradientboostingregressor__alpha':[0.1,0.5,0.9]},\n    'ridge':{'ridge__alpha':[0.1,0.5,0.9]},\n    'lasso':{'lasso__alpha':[0.1,0.5,0.9]},\n    'enet':{'elasticnet__alpha':[0.1,0.5,0.9]}\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "a43dc185-d30b-4304-90e6-15c7c40d2a83"}, "cell_type": "markdown", "source": "### Build and Train Pipelines"}, {"metadata": {"id": "04ad16d5-ef7c-4464-86df-caed1e866241"}, "cell_type": "code", "source": "fit_models = {}\nfor algo, pipeline in pipelines.items():\n    mod = GridSearchCV(pipeline, hyper[algo], n_jobs=-1, cv=10)\n    mod.fit(X_train, y_train)\n    fit_models[algo] = mod\n    print('{} model has been fit'.format(algo))", "execution_count": null, "outputs": []}, {"metadata": {"id": "597167a3f007404f8eb8186f70da51e9"}, "cell_type": "markdown", "source": "<a id='evaluation'></a>\n# 5. Evaluation"}, {"metadata": {"id": "ab75508a-eb3f-4dd5-857b-188c47a58c87"}, "cell_type": "markdown", "source": "### Evaluate Models"}, {"metadata": {"id": "57a59ff7-3adb-457c-b5ef-88346cc59c6c"}, "cell_type": "code", "source": "from sklearn.metrics import r2_score, mean_absolute_error", "execution_count": null, "outputs": []}, {"metadata": {"id": "0c255029-f13b-4884-bf7f-6586aab4c566"}, "cell_type": "code", "source": "for algo, fit_model in fit_models.items():\n    mod = fit_model\n    yhat = mod.predict(X_test)\n    print('Scores for {}, R2: {}, MAE:{}'.format(algo, r2_score(y_test,yhat), mean_absolute_error(y_test,yhat)))", "execution_count": null, "outputs": []}, {"metadata": {"id": "5a270204-e975-419d-beff-79ab51c25be0"}, "cell_type": "markdown", "source": "### Plot Results"}, {"metadata": {"id": "772a85f0-65e8-47af-a873-e8b981e58c6b"}, "cell_type": "code", "source": "viz = pivot_readings_df.dropna()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true, "id": "2be2c8c0-5ccf-4aed-bae3-7dadb52ede81"}, "cell_type": "code", "source": "rf = fit_models['rf'].predict(viz.drop(['timestamp', 'Wheel Front Temp Celsius'], axis=1))\ngb = fit_models['gb'].predict(viz.drop(['timestamp', 'Wheel Front Temp Celsius'], axis=1))\nlasso = fit_models['lasso'].predict(viz.drop(['timestamp', 'Wheel Front Temp Celsius'], axis=1))", "execution_count": null, "outputs": []}, {"metadata": {"id": "01507422-d505-4899-8483-57139c2917dc"}, "cell_type": "code", "source": "viz['rf_pred'] = rf\nviz['gb_pred'] = gb\nviz['lasso_pred'] = lasso", "execution_count": null, "outputs": []}, {"metadata": {"id": "b4391b9f-3656-449a-85ed-1a7932de9f0b"}, "cell_type": "code", "source": "viz.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "f96759d6-2727-47c9-a4c3-0906845e44df"}, "cell_type": "markdown", "source": "Once you have the dataframe with these columns, plot the values. Try to improve the visualization by:\n* Choosing a figure size that will fit the width of your screen\n* Choose different colours for the plotted points\n* Create a legend that explains the values and colours shown in the chart"}, {"metadata": {"id": "e28b2d29-c1d9-46d9-96b1-a165c8d32cc1"}, "cell_type": "code", "source": "plt.figure(figsize=(15,10))\nplt.plot(viz['Wheel Front Speed RPM'].values,viz['Wheel Front Temp Celsius'].values,'c+')\nplt.plot(viz['Wheel Front Speed RPM'].values,viz['rf_pred'].values,'y', label='Prediction Random Forest')\nplt.plot(viz['Wheel Front Speed RPM'].values,viz['gb_pred'].values,'b', label='Prediction Gradient Boosted')\nplt.plot(viz['Wheel Front Speed RPM'].values,viz['lasso_pred'].values,'g', label='Prediction Lasso')\nplt.legend(loc='upper left')\nplt.xlabel('Wheel Front Speed RPM')\nplt.ylabel('Wheel Front Temperature Celsius')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "e78ab46f-4db4-4c76-8ec8-b2a6b1cc65cc"}, "cell_type": "markdown", "source": "<a id='deployment'></a>\n# 6. Deployment\nWe can now deploy the Gradient Boosted model as the highest performing model. "}, {"metadata": {"id": "a3199d16-5723-411c-ac03-6796d53e3911"}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\nimport json\nimport numpy as np \nimport os ", "execution_count": null, "outputs": []}, {"metadata": {"id": "e58e4b22-62e5-45c6-a1cf-33a270cb763f"}, "cell_type": "code", "source": "url = 'YOUR ENV URL HERE'\ntoken = os.environ['USER_ACCESS_TOKEN']", "execution_count": null, "outputs": []}, {"metadata": {"id": "4207ffb518ab49ea87b194b14ca6e0c8"}, "cell_type": "code", "source": "wml_credentials = {\n    \"token\": token, \n    \"url\": url,\n    \"instance_id\": 'openshift',\n    \"version\": '3.5'\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "f0ee0f4222234e5d838b06bf09862aa5"}, "cell_type": "code", "source": "wml_client = APIClient(wml_credentials)\nwml_client.spaces.list()", "execution_count": null, "outputs": []}, {"metadata": {"id": "dd722f70e51047188f48947ffc4e0bed"}, "cell_type": "code", "source": "SPACE_ID=\"YOUR SPACE ID\"\nwml_client.set.default_space(SPACE_ID)", "execution_count": null, "outputs": []}, {"metadata": {"id": "c6e7f65c-c08a-4e4e-abb8-d17c82861589"}, "cell_type": "code", "source": "MODEL_NAME = \"IOT Forecast\"\nDEPLOYMENT_NAME = \"IOT Forecast\"\nBEST_MODEL = fit_models['gb']", "execution_count": null, "outputs": []}, {"metadata": {"id": "2352ae258f0a414fa27bf1cfd5346699"}, "cell_type": "code", "source": "# Set Python Version\nsoftware_spec_uid = wml_client.software_specifications.get_id_by_name('default_py3.7')\n\n# Setup model meta\nmodel_props = {\n    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME, \n    wml_client.repository.ModelMetaNames.TYPE: 'scikit-learn_0.23', \n    wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid \n}\n\n#Save model\nmodel_details = wml_client.repository.store_model(\n    model=BEST_MODEL, \n    meta_props=model_props, \n    training_data=X_train.head(), \n    training_target=y_train.head()\n)", "execution_count": null, "outputs": []}, {"metadata": {"id": "e57163bafaa44963871815dc7603ffe9"}, "cell_type": "code", "source": "model_details", "execution_count": null, "outputs": []}, {"metadata": {"id": "f1c131749e5c46be87068dad1e8dccc3"}, "cell_type": "code", "source": "model_uid = wml_client.repository.get_model_uid(model_details); model_uid", "execution_count": null, "outputs": []}, {"metadata": {"id": "4e7c9996669f4310880915e93133a963"}, "cell_type": "code", "source": "# Set meta\ndeployment_props = {\n    wml_client.deployments.ConfigurationMetaNames.NAME:DEPLOYMENT_NAME, \n    wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\n# Deploy\ndeployment = wml_client.deployments.create(\n    artifact_uid=model_uid, \n    meta_props=deployment_props \n)\n\n# Output result\ndeployment", "execution_count": null, "outputs": []}, {"metadata": {"id": "11b4639699934d19b6e8e417a7b3641e"}, "cell_type": "code", "source": "wml_client.deployments.list()", "execution_count": null, "outputs": []}, {"metadata": {"id": "0d26cddf-43d0-4161-b57e-21ebd25e3abf"}, "cell_type": "markdown", "source": "## Scoring"}, {"metadata": {"id": "847f56fcbe3c44758211647af21766e9"}, "cell_type": "code", "source": "deployment", "execution_count": null, "outputs": []}, {"metadata": {"id": "1ee80bf725bd4a6596f518af33ecffbd"}, "cell_type": "code", "source": "deployment_uid = wml_client.deployments.get_uid(deployment)\npayload = {\"input_data\":\n           [\n               {\"fields\":X_train.columns.to_numpy().tolist(), \"values\":X_train.to_numpy().tolist()}\n           ]\n          }", "execution_count": null, "outputs": []}, {"metadata": {"id": "72d738afe7b74e79a9f856486b8a5a23"}, "cell_type": "code", "source": "result = wml_client.deployments.score(deployment_uid, payload); result", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}